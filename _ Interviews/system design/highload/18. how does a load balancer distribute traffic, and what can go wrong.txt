ðŸ‘‰ A load balancer routes incoming requests across multiple service instances to
spread load and improve availability.

How traffic is distributed:
* Round-robin â€” requests sent evenly in order
* Least connections â€” sends traffic to the least busy instance
* Hash-based â€” based on IP, headers, or cookies (sticky sessions)
* Consistent hashing â€” minimizes remapping when instances are added/removed
* Weighted â€” instances get traffic proportional to capacity

How load balancers are physically implemented:
* Software â€” Nginx, HAProxy
* Cloud-managed â€” AWS ALB/NLB, GCP Load Balancer, Azure Front Door
* In-cluster â€” Kubernetes Service
* DNS-based â€” traffic distributed via DNS (latency-based, geo-based)

What a load balancer relies on:
* Health checks to detect unhealthy instances
* Timeouts and retries
* Connection reuse (keep-alive)

Common things that go wrong:
* Uneven load â€” slow instances still receive traffic
* Sticky sessions / hashing â€” reduce rebalancing flexibility
* Slow health checks â€” traffic sent to dying instances
* Retry storms â€” retries amplify load during failures
* Connection imbalance â€” some instances overloaded, others idle

Under high RPS:
* Load balancer itself can become a bottleneck
* Misconfigured timeouts cause cascading failures
* Network limits (ports, NAT, TLS) are hit

Key insight:
* Load balancing is about fairness and failure handling, not just routing

One-line interview answer:
> A load balancer distributes requests using strategies like round-robin,
least-connections, or consistent hashing, and can be implemented as software,
managed cloud services, hardware appliances, or in-cluster components; poor
health checks, sticky routing, retries, or uneven instance performance can cause
overload and cascading failures.
