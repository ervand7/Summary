ğŸ‘‰ In production Go services, rate limiting is usually a combination of:
(1) per-endpoint or per-user rate limits,
(2) per-instance concurrency caps,
and optionally (3) a shared store for global fairness.

What you typically implement (production baseline):

1ï¸âƒ£ Service middleware / gRPC interceptor
* HTTP: middleware around handlers
* gRPC: unary interceptor
* Extract key: userID / apiKey / IP / orgID
* Choose policy by route: /cards/issue stricter than /cards/list


2ï¸âƒ£ Worker pool / semaphore (per-instance, in-memory)
Example:
* Max 200 concurrent requests per pod

Why:
* Fastest possible protection
* Prevents goroutine, memory, and connection pool exhaustion


3ï¸âƒ£ Global fairness (shared store, e.g. Redis) â€” only if required
Example:
* â€œ100 req/min per apiKey across all podsâ€

How:
* Use atomic operations in a shared store to ensure consistent limits
  across instances

Why:
* Prevents a single client from bypassing limits by hitting multiple pods
* Required for multi-tenant or paid APIs


What production-grade behavior includes:

âœ… Correct response semantics
* Return 429 Too Many Requests for rate limits
* Add headers: Retry-After, X-RateLimit-Limit/Remaining/Reset (optional)

âœ… Failure policy
* If the shared store is unavailable:
  - fail-open for non-critical endpoints
  - fail-closed for expensive or abuse-sensitive operations

âœ… Observability
* metrics: allowed_total, denied_total, limiter_errors_total, limiter_latency_ms
* logs: include key type (user/apiKey/IP), route, and limit tier

âœ… Config & rollout
* Config-driven limits (per route / per tier)
* Ability to adjust limits without redeploys (optional but common)


Minimal Go implementation shape (physically):

* HTTP:
  handler â†’ middleware â†’ (local concurrency limiter)
           â†’ (optional shared-store limiter) â†’ handler

* gRPC:
  unary interceptor â†’ same checks â†’ handler


One-line interview answer:
> Iâ€™d implement rate limiting as middleware or interceptors with a fast
in-memory concurrency cap and per-key rate limits, and only add a shared
store when true global fairness across instances is required, returning 429
with proper headers and monitoring allow/deny behavior.

> Rate-based limiters return 429 when requests are rejected, while
concurrency or queue-based limiters usually return 503 or delay requests.
