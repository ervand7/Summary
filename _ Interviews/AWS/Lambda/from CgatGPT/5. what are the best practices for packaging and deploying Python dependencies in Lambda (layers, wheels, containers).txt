Goal: make deployments small, repeatable, fast, and compatible with Lambda runtime.

Best practices (Python deps):

1) Use the right build environment
   * Build on Amazon Linux compatible env (or use Docker) so compiled deps match Lambda
   * Prefer manylinux wheels when possible

2) Prefer wheels over source builds
   * Use prebuilt wheels to avoid slow/fragile compilation
   * Pin versions for reproducibility

3) Split â€œapp codeâ€ vs â€œshared depsâ€
   * Function ZIP: your handler + small app code
   * Layers: shared libraries used by many Lambdas

Lambda Layers (when to use):
* Great for:
  - shared internal utils
  - common libs used by many functions
  - reducing duplicate deployment sizes
* Keep layers stable and versioned

4) Keep package size under control
   * Avoid bundling AWS SDK if runtime already has it (boto3 is usually present)
   * Remove tests/docs/__pycache__
   * Only include required extras (no â€œdevâ€ deps)

5) Use containers when:
   * You have heavy native deps (numpy/scipy, headless browsers, etc.)
   * You need full control over OS libs
   * You want one artifact for CI/CD across envs

Container best practices:
* Use AWS base images for Lambda
* Multi-stage builds to keep images small
* Pin OS + Python + deps versions

6) CI/CD hygiene
   * Deterministic builds (lock file: requirements.txt w/ hashes or poetry.lock)
   * Separate build step from deploy step
   * Scan artifacts for vulnerabilities (esp. containers)

Interview answer to â€œHow do you package Python deps for Lambda?â€

> I usually package most dependencies as wheels built in an Amazon Linux compatible
environment, keep the function ZIP minimal, and use Lambda Layers for shared libs.
If native dependencies are heavy or need system libraries, I ship a Lambda container
image built from the official AWS base image with multi-stage builds.

ðŸ‘‰ Think: `wheels + slim zip`, `layers for reuse`, `containers for heavy native deps`.
