1. **Что, если сообщение потеряется? Исчезнут ли данные?**
   Kafka поддерживает репликацию для защиты данных. Если настроены `acks=all` и `min.insync.replicas`,
   данные не будут потеряны, даже при сбое брокера.

2. **Что, если сообщение придёт несколько раз? Как избежать дублирования?**  
   Kafka может гарантировать **как минимум один раз** доставку, но не защищает от дублирования.
   Для строгого контроля дубликатов рекомендуется реализовать проверку уникальности на стороне
   потребителя.

3. **Что произойдет, если упадет брокер-лидер партиции?**  
   Kafka Controller переназначит нового лидера среди реплик (ISR), чтобы поддерживать
   доступность.

4. **Можно ли повторно прочитать сообщения после их обработки?**  
   Да, Kafka сохраняет сообщения до истечения TTL. Потребитель может изменить offset и прочитать
   данные повторно.

5. **Как избежать потерь данных при записи продюсером?**  
   Используйте `acks=all` и `min.insync.replicas`. Продюсер будет ждать подтверждения от
   всех синхронизированных реплик перед завершением отправки.

6. **Как Kafka обрабатывает большие объёмы сообщений?**  
   Kafka разделяет топики на партиции, позволяя параллельную обработку сообщений разными
   консюмерами, что увеличивает производительность.

7. **Что такое offset, и зачем он нужен?**  
   Offset — это уникальный идентификатор для каждого сообщения в партиции. Он позволяет
   консюмерам отслеживать, какие сообщения были прочитаны.

8. **Как консумеры обрабатывают сбои?**  
   В случае сбоя консумер может возобновить работу с последнего закоммиченного offset'а
   в группе консумеров.

9. **Что будет, если продюсер завершит отправку без подтверждения (acks=0)?**  
   При `acks=0` продюсер отправит сообщение без ожидания подтверждения, что повышает риск
   потери сообщения.

10. **Какие гарантии доставки обеспечивает Kafka?**  
    Kafka поддерживает **как минимум один раз** доставку по умолчанию, а **ровно один раз**
    можно достичь с помощью `idempotent` продюсеров и транзакций.
